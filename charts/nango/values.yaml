## @param nameOverride String to partially override common.names.name
##
nameOverride: ""
## @param fullnameOverride String to fully override common.names.fullname
##
fullnameOverride: ""
## @param namespaceOverride String to fully override common.names.namespace
##
namespaceOverride: ""
## @param commonLabels Labels to add to all deployed objects
##
commonLabels: {}
## @param commonAnnotations Annotations to add to all deployed objects
##
commonAnnotations: {}

global:
  #Global ontainer image values
  image:
    registry: "nangohq"
    repository: "nango"
    tag: "latest"
    digest: ""
    pullPolicy: "IfNotPresent"
    pullSecrets: []
  defaultStorageClass: ""
  
server:
  name: server
  url: http://nango-server-default.nango
  publicUrl: https://nango-server-default.dev
  image: {}
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: false
    ## @param serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param serviceAccount.automountServiceAccountToken Automount service account token for the server service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param networkPolicy.allowExternal Don't require server label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## server label will have network access to the ports server is listening
    ## on. When true, server will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels: {}
    ## @param networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ## @param networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## %%MAIN_CONTAINER_NAME%% ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param ingress.enabled Enable ingress record generation for %%MAIN_CONTAINER_NAME%%
    ##
    enabled: false
    ## @param ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param ingress.hostname Default host for the ingress record
    ##
    hostname: nango-server-default.dev
    ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
    ## You can:
    ##   - Use the `ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `ingress.selfSigned=true`
    ##
    tls: false
    ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: %%COMPONENT_NAME%%.local
    ##     path: /
    ##
    extraHosts: []
    ## @param ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - %%COMPONENT_NAME%%.local
    ##   secretName: %%COMPONENT_NAME%%.local-tls
    ##
    extraTls: []
    ## @param ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: %%COMPONENT_NAME%%.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param service.type server service type
    ##
    type: LoadBalancer
    ## @param service.ports.http server service HTTP port
    ## @param service.ports.https server service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param service.nodePorts.http Node port for HTTP
    ## @param service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param service.clusterIP server service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param service.loadBalancerIP server service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param service.loadBalancerSourceRanges server service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param service.externalTrafficPolicy server service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param service.annotations Additional custom annotations for server service
    ##
    annotations: {}
    ## @param service.extraPorts Extra ports to expose in server service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param server.containerPorts.http server HTTP container port
    ## @param server.containerPorts.https server HTTPS container port
    ##
  containerPorts:
    http: 80
    https: 443
  ## @param server.extraContainerPorts Optionally specify extra list of additional ports for server containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param server.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param server.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param server.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `server.pdb.minAvailable` and `server.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: false
    minAvailable: ""
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param server.autoscaling.hpa.enabled Enable HPA for server pods
    ## @param server.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param server.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param server.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param server.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: false
      minReplicas: 1
      maxReplicas: 3
      targetCPU: 75
      targetMemory: ""
    ## @param server.autoscaling.vpa.enabled Enable VPA for server pods
    ## @param server.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param server.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param server.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param server.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param server.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param server.sidecars Add additional sidecar containers to the server pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## server resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param server.resourcesPreset Set server container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if server.resources is set (server.resources is recommended for production).
  ##
  resourcesPreset: "nano"
  ## @param server.resources Set server container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param server.extraEnvVars Array with extra environment variables to add to server containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param server.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for server containers
  ##
  extraEnvVarsCM: ""
  ## @param server.extraEnvVarsSecret Name of existing Secret containing extra env vars for server containers
  ##
  extraEnvVarsSecret: ""
  ## Configure extra options for server containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param server.livenessProbe.enabled Enable livenessProbe on server containers
  ## @param server.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param server.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param server.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param server.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param server.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: foo
    periodSeconds: bar
    timeoutSeconds: foo
    failureThreshold: bar
    successThreshold: foo
  ## @param server.readinessProbe.enabled Enable readinessProbe on server containers
  ## @param server.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param server.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param server.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param server.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param server.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: foo
    periodSeconds: bar
    timeoutSeconds: foo
    failureThreshold: bar
    successThreshold: foo
  ## @param server.startupProbe.enabled Enable startupProbe on server containers
  ## @param server.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param server.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param server.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param server.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param server.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: foo
    periodSeconds: bar
    timeoutSeconds: foo
    failureThreshold: bar
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param server.podSecurityContext.enabled Enable server pods' Security Context
  ## @param server.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for server pods
  ## @param server.podSecurityContext.sysctls Set kernel settings using the sysctl interface for server pods
  ## @param server.podSecurityContext.supplementalGroups Set filesystem extra groups for server pods
  ## @param server.podSecurityContext.fsGroup Set fsGroup in server pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param server.deploymentAnnotations Annotations for server deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: true
    ## @param persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/server/data
    ## @param persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 8Gi
    ## @param persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: ""
    ## @param persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param rbac.create Specifies whether RBAC resources should be created
    ##
    create: false
    ## @param rbac.rules Custom RBAC rules to set
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules: []

persist:
  name: persist
  url: http://nango-persist-default.nango
  port: "3007"

orchestrator:
  name: orchestrator
  url: http://nango-orchestrator-default.nango
  port: "3008"

jobs:
  name: jobs
  url: http://nango-jobs-default.nango
  port: "3005"
  serviceAccount:
    create: true
    name: nango-jobs-default-sa
  # clusterAdmin: false -> this should be handled by the above
  persistence:
    enabled: true
    name: nango-jobs-default-pvc
    mountPath: /flows
    storageClass: ""
    size: "1Gi"
  
runner:
  name: runner
  url: http://nango-runner-default.nango
  port: "3006"

redis:  
  enabled: true
  primary:
    persistence:
      enabled: false
    resources:
      limits:
        cpu: "1000m"
        memory: "2048Mi"
      requests:
        cpu: "250m"
        memory: "1024Mi"

postgresql:
  namespace: nango
  enabled: true
  primary:
    persistence:
      enabled: false
    resources:
      limits:
        cpu: "1000m"
        memory: "2048Mi"
      requests:
        cpu: "250m"
        memory: "1024Mi"
  auth:
    postgresPassword: nango
    database: nango

elasticsearch:
  namespace: nango
  enabled: false
  clusterName: elastic
  # single node cluster
  master:
    masterOnly: false
    replicaCount: 1
  data:
    replicaCount: 0
  coordinating:
    replicaCount: 0
  ingest:
    replicaCount: 0
  security:
    enabled: true
    elasticPassword: nango