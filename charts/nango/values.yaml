## @section Common parameters

## @param nameOverride String to partially override common.names.name
##
nameOverride: ""
## @param fullnameOverride String to fully override common.names.fullname
##
fullnameOverride: ""
## @param namespaceOverride String to fully override common.names.namespace
##
namespaceOverride: "nango"
## @param commonLabels Labels to add to all deployed objects
##
commonLabels: {}
## @param commonAnnotations Annotations to add to all deployed objects
##
commonAnnotations: {}

## @section Global Parameters
##

global:
  ## global image
  ## @param global.image.registry [default: nangohq] global image registry
  ## @param global.image.repository [default: nango] global image repository
  ## @skip global.image.tag global image tag (immutable tags are recommended)
  ## @param global.image.digest global image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag image tag (immutable tags are recommended)
  ## @param global.image.pullPolicy global image pull policy
  ## @param global.image.pullSecrets global image pull secrets
  ##
  image:
    registry: "nangohq"
    repository: "nango"
    tag: "latest"
    digest: ""
    pullPolicy: "IfNotPresent"
    pullSecrets: []
  ## @param global.defaultStorageClass Global default StorageClass for Persistent Volume(s)
  ##
  defaultStorageClass: ""

## @section Server Parameters
##
  
server:
  ## @param server.name server name
  ##
  name: server
  ## server image
  ## @param server.image.registry [default: nangohq] server image registry
  ## @param server.image.repository [default: nango] server image repository
  ## @skip server.image.tag server image tag (immutable tags are recommended)
  ## @param server.image.digest server image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag image tag (immutable tags are recommended)
  ## @param server.image.pullPolicy server image pull policy
  ## @param server.image.pullSecrets server image pull secrets
  ##
  image:
    registry: ""
    repository: ""
    tag: ""
    digest: ""
    pullPolicy: ""
    pullSecrets: []
  ## @param server.replicaCount Number of server replicas to deploy
  ##
  replicaCount: 1
  ## @param server.command Override default server container command (useful when using custom images)
  ##
  command: []
  ## @param server.args Override default server container args (useful when using custom images)
  ##
  args:
    - "node"
    - "packages/server/dist/server.js"
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param server.serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: true
    ## @param server.serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the server.names.fullname template
    ##
    name: ""
    ## @param server.serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param server.serviceAccount.automountServiceAccountToken Automount service account token for the server service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param server.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param server.networkPolicy.allowExternal Don't require server label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## server label will have network access to the ports server is listening
    ## on. When true, server will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param server.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param server.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `server.networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param server.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param server.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param server.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `server.networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels: {}
    ## @param server.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `server.networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ## @param server.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `server.networkPolicy.allowExternal` is true.
    ##
    ingressNSPodMatchLabels: {}
  ## server ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param server.ingress.enabled Enable ingress record generation for server
    ##
    enabled: true
    ## @param server.ingress.pathType Ingress path type
    ##
    pathType: Prefix
    ## @param server.ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param server.ingress.hostname Default host for the ingress record
    ##
    hostname: example-app.nango.dev
    ## @param server.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param server.ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param server.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param server.ingress.tls Enable TLS configuration for the host defined at `server.ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.server.ingress.hostname }}`
    ## You can:
    ##   - Use the `server.ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `server.ingress.selfSigned=true`
    ##
    tls: false
    ## @param server.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param server.ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: server.local
    ##     path: /
    ##
    extraHosts: []
    ## @param server.ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param server.ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - server.local
    ##   secretName: server.local-tls
    ##
    extraTls: []
    ## @param server.ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: server.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param server.ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param server.service.type server service type
    ##
    type: LoadBalancer
    ## @param server.service.ports.http server service HTTP port
    ## @param server.service.ports.https server service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param server.service.nodePorts.http Node port for HTTP
    ## @param server.service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param server.service.clusterIP server service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param server.service.loadBalancerIP server service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param server.service.loadBalancerSourceRanges server service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param server.service.externalTrafficPolicy server service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param server.service.annotations Additional custom annotations for server service
    ##
    annotations: {}
    ## @param server.service.extraPorts Extra ports to expose in server service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param server.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param server.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
  ## @param server.containerPorts.http server HTTP container port
  ## @param server.containerPorts.https server HTTPS container port
  ##
  containerPorts:
    http: 8080
    https: 8080
  ## @param server.extraContainerPorts Optionally specify extra list of additional ports for server containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  ## @param server.updateStrategy.type server deployment strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  ## @param server.updateStrategy.type server statefulset strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param server.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param server.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param server.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `server.pdb.minAvailable` and `server.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: 1
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param server.autoscaling.hpa.enabled Enable HPA for server pods
    ## @param server.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param server.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param server.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param server.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: true
      minReplicas: 6
      maxReplicas: 12
      targetCPU: 70
      targetMemory: 70
    ## @param server.autoscaling.vpa.enabled Enable VPA for server pods
    ## @param server.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param server.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param server.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param server.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param server.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param server.sidecars Add additional sidecar containers to the server pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## server resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param server.resourcesPreset Set server container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if server.resources is set (server.resources is recommended for production).
  ##
  resourcesPreset: "2xlarge"
  ## @param server.resources Set server container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param server.extraEnvVars Array with extra environment variables to add to server containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param server.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for server containers
  ##
  extraEnvVarsCM: ""
  ## @param server.extraEnvVarsCMs Name of existing ConfigMaps containing extra env vars for server containers
  ##
  extraEnvVarsCMs: []
  ## @param server.extraEnvVarsSecret Name of existing Secret containing extra env vars for server containers
  ##
  extraEnvVarsSecret: ""
  ## @param server.extraEnvVarsSecrets Name of existing Secrets containing extra env vars for server containers
  ##
  extraEnvVarsSecrets: []
  ## Configure extra options for server containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param server.livenessProbe.enabled Enable livenessProbe on server containers
  ## @param server.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param server.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param server.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param server.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param server.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1
  ## @param server.readinessProbe.enabled Enable readinessProbe on server containers
  ## @param server.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param server.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param server.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param server.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param server.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 2
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  ## @param server.startupProbe.enabled Enable startupProbe on server containers
  ## @param server.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param server.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param server.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param server.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param server.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 24
    successThreshold: 1
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param server.podSecurityContext.enabled Enable server pods' Security Context
  ## @param server.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for server pods
  ## @param server.podSecurityContext.sysctls Set kernel settings using the sysctl interface for server pods
  ## @param server.podSecurityContext.supplementalGroups Set filesystem extra groups for server pods
  ## @param server.podSecurityContext.fsGroup Set fsGroup in server pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param server.deploymentAnnotations Annotations for server deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param server.persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: false
    ## @param server.persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/server/data
    ## @param server.persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param server.persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param server.persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param server.persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param server.persistence.size Size of data volume
    ##
    size: 8Gi
    ## @param server.persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param server.persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: "nango-jobs"
    ## @param server.persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param server.rbac.create Specifies whether RBAC resources should be created
    ##
    create: false
    ## @param server.rbac.rules Custom RBAC rules to set
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules: []

## @section Persist Parameters
##

persist:
  ## @param persist.name persist name
  ##
  name: persist
  ## persist image
  ## @param persist.image.registry [default: nangohq] persist image registry
  ## @param persist.image.repository [default: nango] persist image repository
  ## @skip persist.image.tag persist image tag (immutable tags are recommended)
  ## @param persist.image.digest persist image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag image tag (immutable tags are recommended)
  ## @param persist.image.pullPolicy persist image pull policy
  ## @param persist.image.pullSecrets persist image pull secrets
  ##
  image:
    registry: ""
    repository: ""
    tag: ""
    digest: ""
    pullPolicy: ""
    pullSecrets: []
  
  ## @param persist.replicaCount Number of persist replicas to deploy
  ##
  replicaCount: 1
  ## @param persist.command Override default persist container command (useful when using custom images)
  ##
  command: []
  ## @param persist.args Override default persist container args (useful when using custom images)
  ##
  args:
    - node
    - "packages/persist/dist/app.js"
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param persist.serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: true
    ## @param persist.serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param persist.serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param persist.serviceAccount.automountServiceAccountToken Automount service account token for the server service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param persist.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param persist.networkPolicy.allowExternal Don't require server label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## server label will have network access to the ports server is listening
    ## on. When true, server will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: false
    ## @param persist.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param persist.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `persist.networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param persist.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param persist.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param persist.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `persist.networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels:
      app.kubernetes.io/component: runner
    ## @param persist.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `persist.networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ## @param persist.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `persist.networkPolicy.allowExternal` is true.
    ##
    ingressNSPodMatchLabels: {}
  ## persist ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param persist.ingress.enabled Enable ingress record generation for persist
    ##
    enabled: false
    ## @param persist.ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param persist.ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param persist.ingress.hostname Default host for the ingress record
    ##
    hostname: nango-server-default.dev
    ## @param persist.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param persist.ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param persist.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param persist.ingress.tls Enable TLS configuration for the host defined at `persist.ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.persist.ingress.hostname }}`
    ## You can:
    ##   - Use the `persist.ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `persist.ingress.selfSigned=true`
    ##
    tls: false
    ## @param persist.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param persist.ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: persist.local
    ##     path: /
    ##
    extraHosts: []
    ## @param persist.ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param persist.ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - persist.local
    ##   secretName: persist.local-tls
    ##
    extraTls: []
    ## @param persist.ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: persist.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param persist.ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param persist.service.type persist service type
    ##
    type: LoadBalancer
    ## @param persist.service.ports.http persist service HTTP port
    ## @param persist.service.ports.https persist service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param persist.service.nodePorts.http Node port for HTTP
    ## @param persist.service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param persist.service.clusterIP persist service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param persist.service.loadBalancerIP persist service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param persist.service.loadBalancerSourceRanges persist service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param persist.service.externalTrafficPolicy persist service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param persist.service.annotations Additional custom annotations for persist service
    ##
    annotations: {}
    ## @param persist.service.extraPorts Extra ports to expose in persist service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param persist.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param persist.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
  ## @param persist.containerPorts.http persist HTTP container port
  ## @param persist.containerPorts.https persist HTTPS container port
  ##
  containerPorts:
    http: 3007
    https: 3007
  ## @param persist.extraContainerPorts Optionally specify extra list of additional ports for persist containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  ## @param persist.updateStrategy.type persist deployment strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  ## @param persist.updateStrategy.type persist statefulset strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param persist.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param persist.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param persist.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `persist.pdb.minAvailable` and `persist.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: 1
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param persist.autoscaling.hpa.enabled Enable HPA for persist pods
    ## @param persist.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param persist.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param persist.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param persist.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: true
      minReplicas: 6
      maxReplicas: 15
      targetCPU: 60
      targetMemory: 70
    ## @param persist.autoscaling.vpa.enabled Enable VPA for persist pods
    ## @param persist.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param persist.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param persist.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param persist.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param persist.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param persist.sidecars Add additional sidecar containers to the persist pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## persist resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param persist.resourcesPreset Set persist container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if persist.resources is set (persist.resources is recommended for production).
  ##
  resourcesPreset: "xlarge"
  ## @param persist.resources Set persist container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param persist.extraEnvVars Array with extra environment variables to add to persist containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param persist.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for persist containers
  ##
  extraEnvVarsCM: ""
  ## @param persist.extraEnvVarsCMs Name of existing ConfigMaps containing extra env vars for persist containers
  ##
  extraEnvVarsCMs: []
  ## @param persist.extraEnvVarsSecret Name of existing Secret containing extra env vars for persist containers
  ##
  extraEnvVarsSecret: ""
  ## @param persist.extraEnvVarsSecrets Name of existing Secrets containing extra env vars for persist containers
  ##
  extraEnvVarsSecrets: []
  ## Configure extra options for persist containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes

  ## @param persist.livenessProbe.enabled Enable readinessProbe on persist containers
  ## @param persist.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param persist.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param persist.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param persist.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param persist.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1
  ## @param persist.readinessProbe.enabled Enable readinessProbe on persist containers
  ## @param persist.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param persist.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param persist.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param persist.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param persist.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 2
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  ## @param persist.startupProbe.enabled Enable startupProbe on persist containers
  ## @param persist.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param persist.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param persist.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param persist.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param persist.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 24
    successThreshold: 1
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param persist.podSecurityContext.enabled Enable persist pods' Security Context
  ## @param persist.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for persist pods
  ## @param persist.podSecurityContext.sysctls Set kernel settings using the sysctl interface for persist pods
  ## @param persist.podSecurityContext.supplementalGroups Set filesystem extra groups for persist pods
  ## @param persist.podSecurityContext.fsGroup Set fsGroup in persist pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param persist.deploymentAnnotations Annotations for persist deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param persist.persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: false
    ## @param persist.persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/server/data
    ## @param persist.persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param persist.persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persist.persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persist.persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persist.persistence.size Size of data volume
    ##
    size: 8Gi
    ## @param persist.persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param persist.persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: ""
    ## @param persist.persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param persist.rbac.create Specifies whether RBAC resources should be created
    ##
    create: false
    ## @param persist.rbac.rules Custom RBAC rules to set
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules: []

## @section Orchestrator Parameters
##

orchestrator:
  ## @param orchestrator.name orchestrator name
  ##
  name: orchestrator
  ## orchestrator image
  ## @param orchestrator.image.registry [default: nangohq] orchestrator image registry
  ## @param orchestrator.image.repository [default: nango] orchestrator image repository
  ## @skip orchestrator.image.tag orchestrator image tag (immutable tags are recommended)
  ## @param orchestrator.image.digest orchestrator image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag image tag (immutable tags are recommended)
  ## @param orchestrator.image.pullPolicy orchestrator image pull policy
  ## @param orchestrator.image.pullSecrets orchestrator image pull secrets
  ##
  image:
    registry: ""
    repository: ""
    tag: ""
    digest: ""
    pullPolicy: ""
    pullSecrets: []
  
  ## @param orchestrator.replicaCount Number of orchestrator replicas to deploy
  ##
  replicaCount: 1
  ## @param orchestrator.command Override default orchestrator container command (useful when using custom images)
  ##
  command: []
  ## @param orchestrator.args Override default orchestrator container args (useful when using custom images)
  ##
  args: 
    - node
    - "packages/orchestrator/dist/app.js"
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param orchestrator.serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: true
    ## @param orchestrator.serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param orchestrator.serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param orchestrator.serviceAccount.automountServiceAccountToken Automount service account token for the orchestrator service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param orchestrator.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param orchestrator.networkPolicy.allowExternal Don't require orchestrator label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## orchestrator label will have network access to the ports orchestrator is listening
    ## on. When true, orchestrator will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param orchestrator.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param orchestrator.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param orchestrator.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param orchestrator.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param orchestrator.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `orchestrator.networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels: {}
    ## @param orchestrator.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `orchestrator.networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ## @param orchestrator.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `orchestrator.networkPolicy.allowExternal` is true.
    ##
    ingressNSPodMatchLabels: {}
  ## orchestrator ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param orchestrator.ingress.enabled Enable ingress record generation for orchestrator
    ##
    enabled: false
    ## @param orchestrator.ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param orchestrator.ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param orchestrator.ingress.hostname Default host for the ingress record
    ##
    hostname: nango-orchestrator-default.dev
    ## @param orchestrator.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param orchestrator.ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param orchestrator.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param orchestrator.ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
    ## You can:
    ##   - Use the `orchestrator.ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `orchestrator.ingress.selfSigned=true`
    ##
    tls: false
    ## @param orchestrator.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param orchestrator.ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: orchestrator.local
    ##     path: /
    ##
    extraHosts: []
    ## @param orchestrator.ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param orchestrator.ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - orchestrator.local
    ##   secretName: orchestrator.local-tls
    ##
    extraTls: []
    ## @param orchestrator.ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: orchestrator.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param orchestrator.ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param orchestrator.service.type orchestrator service type
    ##
    type: LoadBalancer
    ## @param orchestrator.service.ports.http orchestrator service HTTP port
    ## @param orchestrator.service.ports.https orchestrator service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param orchestrator.service.nodePorts.http Node port for HTTP
    ## @param orchestrator.service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param orchestrator.service.clusterIP orchestrator service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param orchestrator.service.loadBalancerIP orchestrator service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param orchestrator.service.loadBalancerSourceRanges orchestrator service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param orchestrator.service.externalTrafficPolicy orchestrator service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param orchestrator.service.annotations Additional custom annotations for orchestrator service
    ##
    annotations: {}
    ## @param orchestrator.service.extraPorts Extra ports to expose in orchestrator service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param orchestrator.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param orchestrator.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
  ## @param orchestrator.containerPorts.http orchestrator HTTP container port
  ## @param orchestrator.containerPorts.https orchestrator HTTPS container port
  ##
  containerPorts:
    http: 3008
    https: 3008
  ## @param orchestrator.extraContainerPorts Optionally specify extra list of additional ports for orchestrator containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  ## @param orchestrator.updateStrategy.type orchestrator deployment strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  ## @param orchestrator.updateStrategy.type orchestrator statefulset strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param orchestrator.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param orchestrator.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param orchestrator.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `orchestrator.pdb.minAvailable` and `orchestrator.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: 1
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param orchestrator.autoscaling.hpa.enabled Enable HPA for orchestrator pods
    ## @param orchestrator.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param orchestrator.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param orchestrator.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param orchestrator.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: true
      minReplicas: 2
      maxReplicas: 3
      targetCPU: 70
      targetMemory: 70
    ## @param orchestrator.autoscaling.vpa.enabled Enable VPA for orchestrator pods
    ## @param orchestrator.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param orchestrator.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param orchestrator.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param orchestrator.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param orchestrator.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param orchestrator.sidecars Add additional sidecar containers to the orchestrator pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## orchestrator resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param orchestrator.resourcesPreset Set orchestrator container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if orchestrator.resources is set (orchestrator.resources is recommended for production).
  ##
  resourcesPreset: "nano"
  ## @param orchestrator.resources Set orchestrator container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param orchestrator.extraEnvVars Array with extra environment variables to add to orchestrator containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param orchestrator.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for orchestrator containers
  ##
  extraEnvVarsCM: ""
  ## @param orchestrator.extraEnvVarsCMs Name of existing ConfigMaps containing extra env vars for orchestrator containers
  ##
  extraEnvVarsCMs: []
  ## @param orchestrator.extraEnvVarsSecret Name of existing Secret containing extra env vars for orchestrator containers
  ##
  extraEnvVarsSecret: ""
  ## @param orchestrator.extraEnvVarsSecrets Name of existing Secrets containing extra env vars for orchestrator containers
  ##
  extraEnvVarsSecrets: []
  ## Configure extra options for orchestrator containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param orchestrator.livenessProbe.enabled Enable livenessProbe on orchestrator containers
  ## @param orchestrator.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param orchestrator.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param orchestrator.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param orchestrator.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param orchestrator.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1
  ## @param orchestrator.readinessProbe.enabled Enable readinessProbe on orchestrator containers
  ## @param orchestrator.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param orchestrator.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param orchestrator.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param orchestrator.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param orchestrator.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 2
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  ## @param orchestrator.startupProbe.enabled Enable startupProbe on orchestrator containers
  ## @param orchestrator.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param orchestrator.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param orchestrator.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param orchestrator.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param orchestrator.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 24
    successThreshold: 1
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param orchestrator.podSecurityContext.enabled Enable orchestrator pods' Security Context
  ## @param orchestrator.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for orchestrator pods
  ## @param orchestrator.podSecurityContext.sysctls Set kernel settings using the sysctl interface for orchestrator pods
  ## @param orchestrator.podSecurityContext.supplementalGroups Set filesystem extra groups for orchestrator pods
  ## @param orchestrator.podSecurityContext.fsGroup Set fsGroup in orchestrator pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param orchestrator.deploymentAnnotations Annotations for orchestrator deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param orchestrator.persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: false
    ## @param orchestrator.persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/orchestrator/data
    ## @param orchestrator.persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param orchestrator.persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param orchestrator.persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param orchestrator.persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param orchestrator.persistence.size Size of data volume
    ##
    size: 1Gi
    ## @param orchestrator.persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param orchestrator.persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: ""
    ## @param orchestrator.persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param orchestrator.rbac.create Specifies whether RBAC resources should be created
    ##
    create: false
    ## @param orchestrator.rbac.rules Custom RBAC rules to set
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules: []

## @section Jobs Parameters
##

jobs:
  ## @param jobs.name jobs name
  ##
  name: jobs
  ## jobs image
  ## @param jobs.image.registry [default: nangohq] jobs image registry
  ## @param jobs.image.repository [default: nango] jobs image repository
  ## @skip jobs.image.tag jobs image tag (immutable tags are recommended)
  ## @param jobs.image.digest jobs image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag image tag (immutable tags are recommended)
  ## @param jobs.image.pullPolicy jobs image pull policy
  ## @param jobs.image.pullSecrets jobs image pull secrets
  ##
  image: 
    registry: ""
    repository: ""
    tag: ""
    digest: ""
    pullPolicy: ""
    pullSecrets: []
  
  ## @param jobs.replicaCount Number of jobs replicas to deploy
  ##
  replicaCount: 1
  ## @param jobs.command Override default jobs container command (useful when using custom images)
  ##
  command: []
  ## @param jobs.args Override default jobs container args (useful when using custom images)
  ##
  args: 
    - node
    - "packages/jobs/dist/app.js"
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param jobs.serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: true
    ## @param jobs.serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the jobs.names.fullname template
    ##
    name: ""
    ## @param jobs.serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param jobs.serviceAccount.automountServiceAccountToken Automount service account token for the jobs service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param jobs.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param jobs.networkPolicy.allowExternal Don't require server label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## server label will have network access to the ports server is listening
    ## on. When true, server will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param jobs.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param jobs.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param jobs.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param jobs.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param jobs.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `jobs.networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels:
      app.kubernetes.io/component: runner
    ## @param jobs.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `jobs.networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ## @param jobs.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `jobs.networkPolicy.allowExternal` is true.
    ##
    ingressNSPodMatchLabels:
  ## jobs ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param jobs.ingress.enabled Enable ingress record generation for jobs
    ##
    enabled: false
    ## @param jobs.ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param jobs.ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param jobs.ingress.hostname Default host for the ingress record
    ##
    hostname: ""
    ## @param jobs.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param jobs.ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param jobs.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param jobs.ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
    ## You can:
    ##   - Use the `jobs.ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `jobs.ingress.selfSigned=true`
    ##
    tls: false
    ## @param jobs.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param jobs.ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: jobs.local
    ##     path: /
    ##
    extraHosts: []
    ## @param jobs.ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param jobs.ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - jobs.local
    ##   secretName: jobs.local-tls
    ##
    extraTls: []
    ## @param jobs.ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: jobs.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param jobs.ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param jobs.service.type jobs service type
    ##
    type: LoadBalancer
    ## @param jobs.service.ports.http jobs service HTTP port
    ## @param jobs.service.ports.https jobs service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param jobs.service.nodePorts.http Node port for HTTP
    ## @param jobs.service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param jobs.service.clusterIP jobs service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param jobs.service.loadBalancerIP jobs service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param jobs.service.loadBalancerSourceRanges jobs service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param jobs.service.externalTrafficPolicy jobs service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param jobs.service.annotations Additional custom annotations for jobs service
    ##
    annotations: {}
    ## @param jobs.service.extraPorts Extra ports to expose in jobs service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param jobs.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param jobs.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
  ## @param jobs.containerPorts.http jobs HTTP container port
  ## @param jobs.containerPorts.https jobs HTTPS container port
  ##
  containerPorts:
    http: 3005
    https: 3005
  ## @param jobs.extraContainerPorts Optionally specify extra list of additional ports for jobs containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  ## @param jobs.updateStrategy.type jobs deployment strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  ## @param jobs.updateStrategy.type jobs statefulset strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param jobs.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param jobs.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param jobs.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `jobs.pdb.minAvailable` and `jobs.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: 1
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param jobs.autoscaling.hpa.enabled Enable HPA for jobs pods
    ## @param jobs.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param jobs.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param jobs.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param jobs.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: true
      minReplicas: 3
      maxReplicas: 6
      targetCPU: 90
      targetMemory: 70
    ## @param jobs.autoscaling.vpa.enabled Enable VPA for jobs pods
    ## @param jobs.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param jobs.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param jobs.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param jobs.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param jobs.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param jobs.sidecars Add additional sidecar containers to the jobs pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## jobs resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param jobs.resourcesPreset Set jobs container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if jobs.resources is set (jobs.resources is recommended for production).
  ##
  resourcesPreset: "2xlarge"
  ## @param jobs.resources Set jobs container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param jobs.extraEnvVars Array with extra environment variables to add to jobs containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param jobs.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for jobs containers
  ##
  extraEnvVarsCM: ""
  ## @param jobs.extraEnvVarsCMs Name of existing ConfigMaps containing extra env vars for jobs containers
  ##
  extraEnvVarsCMs: []
  ## @param jobs.extraEnvVarsSecret Name of existing Secret containing extra env vars for jobs containers
  ##
  extraEnvVarsSecret: ""
  ## @param jobs.extraEnvVarsSecrets Name of existing Secrets containing extra env vars for jobs containers
  ##
  extraEnvVarsSecrets: []
  ## Configure extra options for jobs containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param jobs.livenessProbe.enabled Enable livenessProbe on jobs containers
  ## @param jobs.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param jobs.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param jobs.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param jobs.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param jobs.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1
  ## @param jobs.readinessProbe.enabled Enable readinessProbe on jobs containers
  ## @param jobs.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param jobs.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param jobs.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param jobs.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param jobs.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 2
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  ## @param jobs.startupProbe.enabled Enable startupProbe on jobs containers
  ## @param jobs.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param jobs.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param jobs.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param jobs.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param jobs.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 24
    successThreshold: 1
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param jobs.podSecurityContext.enabled Enable jobs pods' Security Context
  ## @param jobs.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for jobs pods
  ## @param jobs.podSecurityContext.sysctls Set kernel settings using the sysctl interface for jobs pods
  ## @param jobs.podSecurityContext.supplementalGroups Set filesystem extra groups for jobs pods
  ## @param jobs.podSecurityContext.fsGroup Set fsGroup in jobs pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param jobs.deploymentAnnotations Annotations for jobs deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param jobs.persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: false
    ## @param jobs.persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/jobs/data
    ## @param jobs.persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param jobs.persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param jobs.persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param jobs.persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param jobs.persistence.size Size of data volume
    ##
    size: 1Gi
    ## @param jobs.persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param jobs.persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: ""
    ## @param jobs.persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param jobs.rbac.create Specifies whether RBAC resources should be created
    ##
    create: true
    ## @skip jobs.rbac.rules[0]
    ## @skip jobs.rbac.rules[1]
    ## @skip jobs.rbac.rules[2]
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules:
      - apiGroups: 
        - "apps"
        resources: 
        - "deployments"
        verbs: 
        - "get"
        - "list"
        - "watch"
        - "create"
        - "update"
      - apiGroups: 
        - ""
        resources: 
        - "services"
        - "pods"
        - "configmaps"
        - "secrets"
        - "persistentvolumeclaims"
        - "namespaces"
        verbs: 
        - "get"
        - "list"
        - "watch"
        - "create"
        - "update"
        - "patch"
        - "delete"
      - apiGroups: 
        - "networking.k8s.io"
        resources: 
        - "networkpolicies"
        verbs: 
        - "get"
        - "list"
        - "watch"
        - "create"
        - "update"
        - "patch"
        - "delete"
  
## @section Runner Parameters
##

runner:
  ## @param runner.enabled Enable runner deployment
  ##
  enabled: false
  ## @param runner.name runner name
  ##
  name: runner
  ## runner image
  ## @param runner.image.registry [default: nangohq] runner image registry
  ## @param runner.image.repository [default: nango] runner image repository
  ## @skip runner.image.tag runner image tag (immutable tags are recommended)
  ## @param runner.image.digest runner image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag image tag (immutable tags are recommended)
  ## @param runner.image.pullPolicy runner image pull policy
  ## @param runner.image.pullSecrets runner image pull secrets
  ##
  image: 
    registry: ""
    repository: ""
    tag: ""
    digest: ""
    pullPolicy: ""
    pullSecrets: []
  
  ## @param runner.replicaCount Number of runner replicas to deploy
  ##
  replicaCount: 1
  ## @param runner.command Override default runner container command (useful when using custom images)
  ##
  command: []
  ## @param runner.args Override default runner container args (useful when using custom images)
  ##
  args:
    - node
    - "packages/runner/dist/app.js"
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param runner.serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: false
    ## @param runner.serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param runner.serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param runner.serviceAccount.automountServiceAccountToken Automount service account token for the runner service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param runner.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param runner.networkPolicy.allowExternal Don't require runner label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## runner label will have network access to the ports runner is listening
    ## on. When true, runner will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param runner.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param runner.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param runner.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param runner.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param runner.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels:
      app.kubernetes.io/component: jobs
    ## @param runner.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ## @param runner.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## runner ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param runner.ingress.enabled Enable ingress record generation for runner
    ##
    enabled: false
    ## @param runner.ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param runner.ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param runner.ingress.hostname Default host for the ingress record
    ##
    hostname: nango-server-default.dev
    ## @param runner.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param runner.ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param runner.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param runner.ingress.tls Enable TLS configuration for the host defined at `runner.ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.runner.ingress.hostname }}`
    ## You can:
    ##   - Use the `runner.ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `runner.ingress.selfSigned=true`
    ##
    tls: false
    ## @param runner.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param runner.ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: runner.local
    ##     path: /
    ##
    extraHosts: []
    ## @param runner.ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param runner.ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - runner.local
    ##   secretName: runner.local-tls
    ##
    extraTls: []
    ## @param runner.ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: runner.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param runner.ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param runner.service.type runner service type
    ##
    type: LoadBalancer
    ## @param runner.service.ports.http runner service HTTP port
    ## @param runner.service.ports.https runner service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param runner.service.nodePorts.http Node port for HTTP
    ## @param runner.service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param runner.service.clusterIP runner service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param runner.service.loadBalancerIP runner service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param runner.service.loadBalancerSourceRanges runner service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param runner.service.externalTrafficPolicy runner service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param runner.service.annotations Additional custom annotations for runner service
    ##
    annotations: {}
    ## @param runner.service.extraPorts Extra ports to expose in runner service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param runner.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param runner.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
  ## @param runner.containerPorts.http runner HTTP container port
  ## @param runner.containerPorts.https runner HTTPS container port
  ##
  containerPorts:
    http: 3006
    https: 3006
  ## @param runner.extraContainerPorts Optionally specify extra list of additional ports for runner containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  ## @param runner.updateStrategy.type runner deployment strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  ## @param runner.updateStrategy.type runner statefulset strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param runner.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param runner.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param runner.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `runner.pdb.minAvailable` and `runner.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: false
    minAvailable: ""
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param runner.autoscaling.hpa.enabled Enable HPA for runner pods
    ## @param runner.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param runner.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param runner.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param runner.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: true
      minReplicas: 1
      maxReplicas: 3
      targetCPU: 75
      targetMemory: 75
    ## @param runner.autoscaling.vpa.enabled Enable VPA for runner pods
    ## @param runner.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param runner.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param runner.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param runner.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param runner.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param runner.sidecars Add additional sidecar containers to the runner pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## runner resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param runner.resourcesPreset Set runner container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if runner.resources is set (runner.resources is recommended for production).
  ##
  resourcesPreset: "small"
  ## @param runner.resources Set runner container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param runner.extraEnvVars Array with extra environment variables to add to runner containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param runner.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for runner containers
  ##
  extraEnvVarsCM: ""
  ## @param runner.extraEnvVarsCMs Name of existing ConfigMaps containing extra env vars for runner containers
  ##
  extraEnvVarsCMs: []
  ## @param runner.extraEnvVarsSecret Name of existing Secret containing extra env vars for runner containers
  ##
  extraEnvVarsSecret: ""
  ## @param runner.extraEnvVarsSecrets Name of existing Secrets containing extra env vars for runner containers
  ##
  extraEnvVarsSecrets: []
  ## Configure extra options for runner containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param runner.livenessProbe.enabled Enable livenessProbe on runner containers
  ## @param runner.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param runner.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param runner.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param runner.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param runner.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1
  ## @param runner.readinessProbe.enabled Enable readinessProbe on runner containers
  ## @param runner.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param runner.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param runner.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param runner.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param runner.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 2
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  ## @param runner.startupProbe.enabled Enable startupProbe on runner containers
  ## @param runner.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param runner.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param runner.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param runner.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param runner.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 24
    successThreshold: 1
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param runner.podSecurityContext.enabled Enable runner pods' Security Context
  ## @param runner.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for runner pods
  ## @param runner.podSecurityContext.sysctls Set kernel settings using the sysctl interface for runner pods
  ## @param runner.podSecurityContext.supplementalGroups Set filesystem extra groups for runner pods
  ## @param runner.podSecurityContext.fsGroup Set fsGroup in runner pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param runner.deploymentAnnotations Annotations for runner deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param runner.persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: false
    ## @param runner.persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/runner/data
    ## @param runner.persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param runner.persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param runner.persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param runner.persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param runner.persistence.size Size of data volume
    ##
    size: 8Gi
    ## @param runner.persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param runner.persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: ""
    ## @param runner.persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param runner.rbac.create Specifies whether RBAC resources should be created
    ##
    create: false
    ## @param runner.rbac.rules Custom RBAC rules to set
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules: []

## @section Metering Parameters
##

metering:
  ## @param metering.enabled Enable/disable metering
  enabled: false
  ## @param metering.name metering name
  ##
  name: metering
  ## metering image
  ## @param metering.image.registry [default: nangohq] metering image registry
  ## @param metering.image.repository [default: nango] metering image repository
  ## @skip metering.image.tag metering image tag (immutable tags are recommended)
  ## @param metering.image.digest metering image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag image tag (immutable tags are recommended)
  ## @param metering.image.pullPolicy metering image pull policy
  ## @param metering.image.pullSecrets metering image pull secrets
  ##
  image:
    registry: ""
    repository: ""
    tag: ""
    digest: ""
    pullPolicy: ""
    pullSecrets: []
  ## @param metering.replicaCount Number of metering replicas to deploy
  ##
  replicaCount: 1
  ## @param metering.command Override default metering container command (useful when using custom images)
  ##
  command: []
  ## @param metering.args Override default metering container args (useful when using custom images)
  ##
  args:
    - "node"
    - "packages/metering/dist/app.js"
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param metering.serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: true
    ## @param metering.serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the metering.names.fullname template
    ##
    name: ""
    ## @param metering.serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param metering.serviceAccount.automountServiceAccountToken Automount service account token for the metering service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param metering.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param metering.networkPolicy.allowExternal Don't require metering label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## metering label will have network access to the ports metering is listening
    ## on. When true, metering will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param metering.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param metering.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `metering.networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param metering.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param metering.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param metering.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `metering.networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels: {}
    ## @param metering.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `metering.networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ## @param metering.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `metering.networkPolicy.allowExternal` is true.
    ##
    ingressNSPodMatchLabels: {}
  ## metering ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param metering.ingress.enabled Enable ingress record generation for metering
    ##
    enabled: false
    ## @param metering.ingress.pathType Ingress path type
    ##
    pathType: Prefix
    ## @param metering.ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param metering.ingress.hostname Default host for the ingress record
    ##
    hostname: example-app.nango.dev
    ## @param metering.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param metering.ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param metering.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param metering.ingress.tls Enable TLS configuration for the host defined at `metering.ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.metering.ingress.hostname }}`
    ## You can:
    ##   - Use the `metering.ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `metering.ingress.selfSigned=true`
    ##
    tls: false
    ## @param metering.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param metering.ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: metering.local
    ##     path: /
    ##
    extraHosts: []
    ## @param metering.ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param metering.ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - metering.local
    ##   secretName: metering.local-tls
    ##
    extraTls: []
    ## @param metering.ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: metering.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param metering.ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param metering.service.create Create metering service
    ##
    create: false
    ## @param metering.service.type metering service type
    ##
    type: LoadBalancer
    ## @param metering.service.ports.http metering service HTTP port
    ## @param metering.service.ports.https metering service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param metering.service.nodePorts.http Node port for HTTP
    ## @param metering.service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param metering.service.clusterIP metering service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param metering.service.loadBalancerIP metering service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param metering.service.loadBalancerSourceRanges metering service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param metering.service.externalTrafficPolicy metering service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param metering.service.annotations Additional custom annotations for metering service
    ##
    annotations: {}
    ## @param metering.service.extraPorts Extra ports to expose in metering service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param metering.service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param metering.service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
  ## @param metering.containerPorts.enabled Enable/disable metering container ports
  ## @param metering.containerPorts.http metering HTTP container port
  ## @param metering.containerPorts.https metering HTTPS container port
  ##
  containerPorts:
    enabled: false
    http: ""
    https: ""
  ## @param metering.extraContainerPorts Optionally specify extra list of additional ports for metering containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  ## @param metering.updateStrategy.type metering deployment strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  ## @param metering.updateStrategy.type metering statefulset strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ##
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param metering.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param metering.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param metering.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `metering.pdb.minAvailable` and `metering.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: 1
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param metering.autoscaling.hpa.enabled Enable HPA for metering pods
    ## @param metering.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param metering.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param metering.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param metering.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: false
      minReplicas: 1
      maxReplicas: 1
      targetCPU: ""
      targetMemory: ""
    ## @param metering.autoscaling.vpa.enabled Enable VPA for metering pods
    ## @param metering.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param metering.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param metering.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param metering.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param metering.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param metering.sidecars Add additional sidecar containers to the metering pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## metering resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param metering.resourcesPreset Set metering container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if metering.resources is set (metering.resources is recommended for production).
  ##
  resourcesPreset: "small"
  ## @param metering.resources Set metering container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param metering.extraEnvVars Array with extra environment variables to add to metering containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param metering.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for metering containers
  ##
  extraEnvVarsCM: ""
  ## @param metering.extraEnvVarsCMs Name of existing ConfigMaps containing extra env vars for metering containers
  ##
  extraEnvVarsCMs: []
  ## @param metering.extraEnvVarsSecret Name of existing Secret containing extra env vars for metering containers
  ##
  extraEnvVarsSecret: ""
  ## @param metering.extraEnvVarsSecrets Name of existing Secrets containing extra env vars for metering containers
  ##
  extraEnvVarsSecrets: []
  ## Configure extra options for metering containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param metering.livenessProbe.enabled Enable livenessProbe on metering containers
  ## @param metering.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param metering.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param metering.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param metering.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param metering.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: false
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1
  ## @param metering.readinessProbe.enabled Enable readinessProbe on metering containers
  ## @param metering.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param metering.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param metering.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param metering.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param metering.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: false
    initialDelaySeconds: 2
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  ## @param metering.startupProbe.enabled Enable startupProbe on metering containers
  ## @param metering.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param metering.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param metering.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param metering.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param metering.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 24
    successThreshold: 1
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param metering.podSecurityContext.enabled Enable metering pods' Security Context
  ## @param metering.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for metering pods
  ## @param metering.podSecurityContext.sysctls Set kernel settings using the sysctl interface for metering pods
  ## @param metering.podSecurityContext.supplementalGroups Set filesystem extra groups for metering pods
  ## @param metering.podSecurityContext.fsGroup Set fsGroup in metering pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param metering.deploymentAnnotations Annotations for metering deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param metering.persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: false
    ## @param metering.persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/metering/data
    ## @param metering.persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param metering.persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param metering.persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param metering.persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param metering.persistence.size Size of data volume
    ##
    size: 8Gi
    ## @param metering.persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param metering.persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: "nango-jobs"
    ## @param metering.persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param metering.rbac.create Specifies whether RBAC resources should be created
    ##
    create: false
    ## @param metering.rbac.rules Custom RBAC rules to set
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules: []


## @skip redis
redis:  
  enabled: false
  
## @skip postgresql
postgresql:
  enabled: false
  
## @skip elasticsearch 
elasticsearch:
  enabled: false