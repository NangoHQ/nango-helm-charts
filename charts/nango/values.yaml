## @param nameOverride String to partially override common.names.name
##
nameOverride: ""
## @param fullnameOverride String to fully override common.names.fullname
##
fullnameOverride: ""
## @param namespaceOverride String to fully override common.names.namespace
##
namespaceOverride: "nango"
## @param commonLabels Labels to add to all deployed objects
##
commonLabels: {}
## @param commonAnnotations Annotations to add to all deployed objects
##
commonAnnotations: {}

global:
  #Global ontainer image values
  image:
    registry: "nangohq"
    repository: "nango"
    tag: "latest"
    digest: ""
    pullPolicy: "IfNotPresent"
    pullSecrets: []
  defaultStorageClass: ""
  
server:
  name: server
  image: {}
  args:
    - node
    - "packages/server/dist/server.js"
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: true
    ## @param serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the server.names.fullname template
    ##
    name: ""
    ## @param serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param serviceAccount.automountServiceAccountToken Automount service account token for the server service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param networkPolicy.allowExternal Don't require server label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## server label will have network access to the ports server is listening
    ## on. When true, server will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels: {}
    ## @param networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ## @param networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## server ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param ingress.enabled Enable ingress record generation for server
    ##
    enabled: true
    ## @param ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param ingress.hostname Default host for the ingress record
    ##
    hostname: example-app.nango.dev
    ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
    ## You can:
    ##   - Use the `ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `ingress.selfSigned=true`
    ##
    tls: false
    ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: server.local
    ##     path: /
    ##
    extraHosts: []
    ## @param ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - server.local
    ##   secretName: server.local-tls
    ##
    extraTls: []
    ## @param ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: server.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param service.type server service type
    ##
    type: LoadBalancer
    ## @param service.ports.http server service HTTP port
    ## @param service.ports.https server service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param service.nodePorts.http Node port for HTTP
    ## @param service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param service.clusterIP server service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param service.loadBalancerIP server service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param service.loadBalancerSourceRanges server service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param service.externalTrafficPolicy server service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param service.annotations Additional custom annotations for server service
    ##
    annotations: {}
    ## @param service.extraPorts Extra ports to expose in server service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param server.containerPorts.http server HTTP container port
    ## @param server.containerPorts.https server HTTPS container port
    ##
  containerPorts:
    http: 8080
    https: 8080
  ## @param server.extraContainerPorts Optionally specify extra list of additional ports for server containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param server.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param server.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param server.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `server.pdb.minAvailable` and `server.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: 1
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param server.autoscaling.hpa.enabled Enable HPA for server pods
    ## @param server.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param server.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param server.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param server.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: true
      minReplicas: 6
      maxReplicas: 12
      targetCPU: 70
      targetMemory: 70
    ## @param server.autoscaling.vpa.enabled Enable VPA for server pods
    ## @param server.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param server.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param server.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param server.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param server.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param server.sidecars Add additional sidecar containers to the server pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## server resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param server.resourcesPreset Set server container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if server.resources is set (server.resources is recommended for production).
  ##
  resourcesPreset: "2xlarge"
  ## @param server.resources Set server container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param server.extraEnvVars Array with extra environment variables to add to server containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param server.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for server containers
  ##
  extraEnvVarsCM: ""
  ## @param server.extraEnvVarsSecret Name of existing Secret containing extra env vars for server containers
  ##
  extraEnvVarsSecret: ""
  ## Configure extra options for server containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param server.livenessProbe.enabled Enable livenessProbe on server containers
  ## @param server.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param server.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param server.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param server.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param server.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1
  ## @param server.readinessProbe.enabled Enable readinessProbe on server containers
  ## @param server.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param server.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param server.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param server.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param server.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 2
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  ## @param server.startupProbe.enabled Enable startupProbe on server containers
  ## @param server.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param server.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param server.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param server.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param server.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 24
    successThreshold: 1
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param server.podSecurityContext.enabled Enable server pods' Security Context
  ## @param server.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for server pods
  ## @param server.podSecurityContext.sysctls Set kernel settings using the sysctl interface for server pods
  ## @param server.podSecurityContext.supplementalGroups Set filesystem extra groups for server pods
  ## @param server.podSecurityContext.fsGroup Set fsGroup in server pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param server.deploymentAnnotations Annotations for server deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: true
    ## @param persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/server/data
    ## @param persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 8Gi
    ## @param persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: "nango-v2-jobs"
    ## @param persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param rbac.create Specifies whether RBAC resources should be created
    ##
    create: false
    ## @param rbac.rules Custom RBAC rules to set
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules: []

persist:
  name: persist
  image: {}
  args:
    - node
    - "packages/persist/dist/app.js"
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: true
    ## @param serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param serviceAccount.automountServiceAccountToken Automount service account token for the server service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param networkPolicy.allowExternal Don't require server label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## server label will have network access to the ports server is listening
    ## on. When true, server will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: false
    ## @param networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels:
      app.kubernetes.io/component: runner
    ## @param networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ## @param networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ##
    ingressNSPodMatchLabels: {}
  ## persist ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param ingress.enabled Enable ingress record generation for persist
    ##
    enabled: false
    ## @param ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param ingress.hostname Default host for the ingress record
    ##
    hostname: nango-server-default.dev
    ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
    ## You can:
    ##   - Use the `ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `ingress.selfSigned=true`
    ##
    tls: false
    ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: persist.local
    ##     path: /
    ##
    extraHosts: []
    ## @param ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - persist.local
    ##   secretName: persist.local-tls
    ##
    extraTls: []
    ## @param ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: persist.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param service.type persist service type
    ##
    type: LoadBalancer
    ## @param service.ports.http persist service HTTP port
    ## @param service.ports.https persist service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param service.nodePorts.http Node port for HTTP
    ## @param service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param service.clusterIP persist service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param service.loadBalancerIP persist service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param service.loadBalancerSourceRanges persist service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param service.externalTrafficPolicy persist service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param service.annotations Additional custom annotations for persist service
    ##
    annotations: {}
    ## @param service.extraPorts Extra ports to expose in persist service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param persist.containerPorts.http persist HTTP container port
    ## @param persist.containerPorts.https persist HTTPS container port
    ##
  containerPorts:
    http: 3007
    https: 3007
  ## @param server.extraContainerPorts Optionally specify extra list of additional ports for server containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param persist.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param persist.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param persist.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `persist.pdb.minAvailable` and `persist.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: 1
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param persist.autoscaling.hpa.enabled Enable HPA for persist pods
    ## @param persist.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param persist.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param persist.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param persist.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: true
      minReplicas: 6
      maxReplicas: 15
      targetCPU: 60
      targetMemory: 70
    ## @param persist.autoscaling.vpa.enabled Enable VPA for persist pods
    ## @param persist.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param persist.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param persist.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param persist.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param persist.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param persist.sidecars Add additional sidecar containers to the persist pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## persist resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param persist.resourcesPreset Set persist container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if persist.resources is set (persist.resources is recommended for production).
  ##
  resourcesPreset: "xlarge"
  ## @param persist.resources Set persist container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param persist.extraEnvVars Array with extra environment variables to add to persist containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param persist.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for persist containers
  ##
  extraEnvVarsCM: ""
  ## @param persist.extraEnvVarsSecret Name of existing Secret containing extra env vars for persist containers
  ##
  extraEnvVarsSecret: ""
  ## Configure extra options for persist containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes

  ## @param persist.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param persist.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param persist.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param persist.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param persist.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1
  ## @param persist.readinessProbe.enabled Enable readinessProbe on persist containers
  ## @param persist.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param persist.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param persist.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param persist.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param persist.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 2
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  ## @param persist.startupProbe.enabled Enable startupProbe on persist containers
  ## @param persist.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param persist.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param persist.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param persist.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param persist.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 24
    successThreshold: 1
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param persist.podSecurityContext.enabled Enable persist pods' Security Context
  ## @param persist.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for persist pods
  ## @param persist.podSecurityContext.sysctls Set kernel settings using the sysctl interface for persist pods
  ## @param persist.podSecurityContext.supplementalGroups Set filesystem extra groups for persist pods
  ## @param persist.podSecurityContext.fsGroup Set fsGroup in persist pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param persist.deploymentAnnotations Annotations for persist deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: false
    ## @param persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/server/data
    ## @param persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 8Gi
    ## @param persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: ""
    ## @param persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param rbac.create Specifies whether RBAC resources should be created
    ##
    create: false
    ## @param rbac.rules Custom RBAC rules to set
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules: []

orchestrator:
  name: orchestrator
  image: {}
  args: 
    - node
    - "packages/orchestrator/dist/app.js"
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: true
    ## @param serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param serviceAccount.automountServiceAccountToken Automount service account token for the orchestrator service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param networkPolicy.allowExternal Don't require orchestrator label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## orchestrator label will have network access to the ports orchestrator is listening
    ## on. When true, orchestrator will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: false
    ## @param networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress:
      - from: 
        - podSelector:
            matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                  - jobs
                  - server
    ## @param networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels: {}
    ## @param networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ## @param networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ##
    ingressNSPodMatchLabels: {}
  ## orchestrator ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param ingress.enabled Enable ingress record generation for orchestrator
    ##
    enabled: false
    ## @param ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param ingress.hostname Default host for the ingress record
    ##
    hostname: nango-orchestrator-default.dev
    ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
    ## You can:
    ##   - Use the `ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `ingress.selfSigned=true`
    ##
    tls: false
    ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: orchestrator.local
    ##     path: /
    ##
    extraHosts: []
    ## @param ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - orchestrator.local
    ##   secretName: orchestrator.local-tls
    ##
    extraTls: []
    ## @param ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: orchestrator.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param service.type orchestrator service type
    ##
    type: LoadBalancer
    ## @param service.ports.http orchestrator service HTTP port
    ## @param service.ports.https orchestrator service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param service.nodePorts.http Node port for HTTP
    ## @param service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param service.clusterIP orchestrator service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param service.loadBalancerIP orchestrator service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param service.loadBalancerSourceRanges orchestrator service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param service.externalTrafficPolicy orchestrator service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param service.annotations Additional custom annotations for orchestrator service
    ##
    annotations: {}
    ## @param service.extraPorts Extra ports to expose in orchestrator service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param orchestrator.containerPorts.http orchestrator HTTP container port
    ## @param orchestrator.containerPorts.https orchestrator HTTPS container port
    ##
  containerPorts:
    http: 3008
    https: 3008
  ## @param orchestrator.extraContainerPorts Optionally specify extra list of additional ports for orchestrator containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param orchestrator.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param orchestrator.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param orchestrator.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `orchestrator.pdb.minAvailable` and `orchestrator.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: 1
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param orchestrator.autoscaling.hpa.enabled Enable HPA for orchestrator pods
    ## @param orchestrator.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param orchestrator.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param orchestrator.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param orchestrator.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: true
      minReplicas: 2
      maxReplicas: 3
      targetCPU: 70
      targetMemory: 70
    ## @param orchestrator.autoscaling.vpa.enabled Enable VPA for orchestrator pods
    ## @param orchestrator.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param orchestrator.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param orchestrator.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param orchestrator.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param orchestrator.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param orchestrator.sidecars Add additional sidecar containers to the orchestrator pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## orchestrator resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param orchestrator.resourcesPreset Set orchestrator container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if orchestrator.resources is set (orchestrator.resources is recommended for production).
  ##
  resourcesPreset: "nano"
  ## @param orchestrator.resources Set orchestrator container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param orchestrator.extraEnvVars Array with extra environment variables to add to orchestrator containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param orchestrator.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for orchestrator containers
  ##
  extraEnvVarsCM: ""
  ## @param orchestrator.extraEnvVarsSecret Name of existing Secret containing extra env vars for orchestrator containers
  ##
  extraEnvVarsSecret: ""
  ## Configure extra options for orchestrator containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param orchestrator.livenessProbe.enabled Enable livenessProbe on orchestrator containers
  ## @param jobs.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param jobs.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param jobs.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param jobs.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param jobs.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1
  ## @param orchestrator.readinessProbe.enabled Enable readinessProbe on orchestrator containers
  ## @param orchestrator.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param orchestrator.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param orchestrator.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param orchestrator.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param orchestrator.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 2
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  ## @param orchestrator.startupProbe.enabled Enable startupProbe on orchestrator containers
  ## @param orchestrator.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param orchestrator.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param orchestrator.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param orchestrator.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param orchestrator.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 24
    successThreshold: 1
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param orchestrator.podSecurityContext.enabled Enable orchestrator pods' Security Context
  ## @param orchestrator.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for orchestrator pods
  ## @param orchestrator.podSecurityContext.sysctls Set kernel settings using the sysctl interface for orchestrator pods
  ## @param orchestrator.podSecurityContext.supplementalGroups Set filesystem extra groups for orchestrator pods
  ## @param orchestrator.podSecurityContext.fsGroup Set fsGroup in orchestrator pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param orchestrator.deploymentAnnotations Annotations for orchestrator deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: false
    ## @param persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/orchestrator/data
    ## @param persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 1Gi
    ## @param persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: ""
    ## @param persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param rbac.create Specifies whether RBAC resources should be created
    ##
    create: false
    ## @param rbac.rules Custom RBAC rules to set
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules: []

jobs:
  name: jobs
  image: {}
  args: 
    - node
    - "packages/jobs/dist/app.js"
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: true
    ## @param serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the jobs.names.fullname template
    ##
    name: ""
    ## @param serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param serviceAccount.automountServiceAccountToken Automount service account token for the jobs service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param networkPolicy.allowExternal Don't require server label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## server label will have network access to the ports server is listening
    ## on. When true, server will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels:
      app.kubernetes.io/component: runner
    ## @param networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ## @param networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ##
    ingressNSPodMatchLabels:
  ## jobs ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param ingress.enabled Enable ingress record generation for jobs
    ##
    enabled: false
    ## @param ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param ingress.hostname Default host for the ingress record
    ##
    hostname: ""
    ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
    ## You can:
    ##   - Use the `ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `ingress.selfSigned=true`
    ##
    tls: false
    ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: jobs.local
    ##     path: /
    ##
    extraHosts: []
    ## @param ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - jobs.local
    ##   secretName: jobs.local-tls
    ##
    extraTls: []
    ## @param ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: jobs.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param service.type jobs service type
    ##
    type: LoadBalancer
    ## @param service.ports.http jobs service HTTP port
    ## @param service.ports.https jobs service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param service.nodePorts.http Node port for HTTP
    ## @param service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param service.clusterIP jobs service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param service.loadBalancerIP jobs service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param service.loadBalancerSourceRanges jobs service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param service.externalTrafficPolicy jobs service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param service.annotations Additional custom annotations for jobs service
    ##
    annotations: {}
    ## @param service.extraPorts Extra ports to expose in jobs service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param jobs.containerPorts.http jobs HTTP container port
    ## @param jobs.containerPorts.https jobs HTTPS container port
    ##
  containerPorts:
    http: 3005
    https: 3005
  ## @param jobs.extraContainerPorts Optionally specify extra list of additional ports for jobs containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param jobs.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param jobs.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param jobs.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `jobs.pdb.minAvailable` and `jobs.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: true
    minAvailable: 1
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param jobs.autoscaling.hpa.enabled Enable HPA for jobs pods
    ## @param jobs.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param jobs.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param jobs.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param jobs.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: true
      minReplicas: 3
      maxReplicas: 6
      targetCPU: 90
      targetMemory: 70
    ## @param jobs.autoscaling.vpa.enabled Enable VPA for jobs pods
    ## @param jobs.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param jobs.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param jobs.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param jobs.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param jobs.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param jobs.sidecars Add additional sidecar containers to the jobs pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## jobs resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param jobs.resourcesPreset Set jobs container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if jobs.resources is set (jobs.resources is recommended for production).
  ##
  resourcesPreset: "2xlarge"
  ## @param jobs.resources Set jobs container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param jobs.extraEnvVars Array with extra environment variables to add to jobs containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param jobs.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for jobs containers
  ##
  extraEnvVarsCM: ""
  ## @param jobs.extraEnvVarsSecret Name of existing Secret containing extra env vars for jobs containers
  ##
  extraEnvVarsSecret: ""
  ## Configure extra options for jobs containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param jobs.livenessProbe.enabled Enable livenessProbe on jobs containers
  ## @param jobs.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param jobs.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param jobs.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param jobs.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param jobs.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1
  ## @param jobs.readinessProbe.enabled Enable readinessProbe on jobs containers
  ## @param jobs.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param jobs.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param jobs.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param jobs.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param jobs.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 2
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  ## @param jobs.startupProbe.enabled Enable startupProbe on jobs containers
  ## @param jobs.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param jobs.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param jobs.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param jobs.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param jobs.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 24
    successThreshold: 1
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param jobs.podSecurityContext.enabled Enable jobs pods' Security Context
  ## @param jobs.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for jobs pods
  ## @param jobs.podSecurityContext.sysctls Set kernel settings using the sysctl interface for jobs pods
  ## @param jobs.podSecurityContext.supplementalGroups Set filesystem extra groups for jobs pods
  ## @param jobs.podSecurityContext.fsGroup Set fsGroup in jobs pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param jobs.deploymentAnnotations Annotations for jobs deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: true
    ## @param persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/jobs/data
    ## @param persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 1Gi
    ## @param persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: ""
    ## @param persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param rbac.create Specifies whether RBAC resources should be created
    ##
    create: true
    ## @param rbac.rules Custom RBAC rules to set
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules:
      - apiGroups: 
        - "apps"
        resources: 
        - "deployments"
        verbs: 
        - "get"
        - "list"
        - "watch"
        - "create"
        - "update"
      - apiGroups: 
        - ""
        resources: 
        - "services"
        - "pods"
        - "configmaps"
        - "secrets"
        - "persistentvolumeclaims"
        - "namespaces"
        verbs: 
        - apiGroups: ["networking.k8s.io"]
        resources: 
        - "networkpolicies"
        verbs: 
        - "get"
        - "list"
        - "watch"
        - "create"
        - "update"
        - "patch"
        - "delete"
  
runner:
  enabled: false
  name: runner
  image: {}
  args:
    - node
    - "packages/runner/dist/app.js"
  ## ServiceAccount configuration
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Specifies whether a ServiceAccount should be created
    ##
    create: false
    ## @param serviceAccount.name The name of the ServiceAccount to use.
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param serviceAccount.annotations Additional Service Account annotations (evaluated as a template)
    ##
    annotations: {}
    ## @param serviceAccount.automountServiceAccountToken Automount service account token for the runner service account
    ##
    automountServiceAccountToken: true
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param networkPolicy.allowExternal Don't require runner label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## runner label will have network access to the ports runner is listening
    ## on. When true, runner will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: false
    ## @param networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: false
    ## @param networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy (ignored if allowExternalEgress=true)
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress:
      - to:
          - podSelector:
              matchExpressions:
                - key: app.kubernetes.io/component
                  operator: In
                  values:
                    - jobs
                    - persist
    ## @param networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels:
      app.kubernetes.io/component: jobs
    ## @param networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ## @param networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## runner ingress parameters
  ## ref: http://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param ingress.enabled Enable ingress record generation for runner
    ##
    enabled: false
    ## @param ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)
    ##
    apiVersion: ""
    ## @param ingress.hostname Default host for the ingress record
    ##
    hostname: nango-server-default.dev
    ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
    ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
    ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
    ##
    ingressClassName: ""
    ## @param ingress.path Default path for the ingress record
    ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
    ##
    path: /
    ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
    ## Use this parameter to set the required annotations for cert-manager, see
    ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
    ## e.g:
    ## annotations:
    ##   kubernetes.io/ingress.class: nginx
    ##   cert-manager.io/cluster-issuer: cluster-issuer-name
    ##
    annotations: {}
    ## @param ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
    ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
    ## You can:
    ##   - Use the `ingress.secrets` parameter to create this TLS secret
    ##   - Rely on cert-manager to create it by setting the corresponding annotations
    ##   - Rely on Helm to create self-signed certificates by setting `ingress.selfSigned=true`
    ##
    tls: false
    ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
    ##
    selfSigned: false
    ## @param ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
    ## e.g:
    ## extraHosts:
    ##   - name: runner.local
    ##     path: /
    ##
    extraHosts: []
    ## @param ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
    ## e.g:
    ## extraPaths:
    ## - path: /*
    ##   backend:
    ##     serviceName: ssl-redirect
    ##     servicePort: use-annotation
    ##
    extraPaths: []
    ## @param ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
    ## e.g:
    ## extraTls:
    ## - hosts:
    ##     - runner.local
    ##   secretName: runner.local-tls
    ##
    extraTls: []
    ## @param ingress.secrets Custom TLS certificates as secrets
    ## NOTE: 'key' and 'certificate' are expected in PEM format
    ## NOTE: 'name' should line up with a 'secretName' set further up
    ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
    ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
    ## It is also possible to create and manage the certificates outside of this helm chart
    ## Please see README.md for more information
    ## e.g:
    ## secrets:
    ##   - name: runner.local-tls
    ##     key: |-
    ##       -----BEGIN RSA PRIVATE KEY-----
    ##       ...
    ##       -----END RSA PRIVATE KEY-----
    ##     certificate: |-
    ##       -----BEGIN CERTIFICATE-----
    ##       ...
    ##       -----END CERTIFICATE-----
    ##
    secrets: []
    ## @param ingress.extraRules Additional rules to be covered with this ingress record
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
    ## e.g:
    ## extraRules:
    ## - host: example.local
    ##     http:
    ##       path: /
    ##       backend:
    ##         service:
    ##           name: example-svc
    ##           port:
    ##             name: http
    ##
    extraRules: []
  service:
    ## @param service.type runner service type
    ##
    type: LoadBalancer
    ## @param service.ports.http runner service HTTP port
    ## @param service.ports.https runner service HTTPS port
    ##
    ports:
      http: 80
      https: 443
    ## Node ports to expose
    ## @param service.nodePorts.http Node port for HTTP
    ## @param service.nodePorts.https Node port for HTTPS
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      http: ""
      https: ""
    ## @param service.clusterIP runner service Cluster IP
    ## e.g.:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param service.loadBalancerIP runner service Load Balancer IP
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
    ##
    loadBalancerIP: ""
    ## @param service.loadBalancerSourceRanges runner service Load Balancer sources
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ## e.g:
    ## loadBalancerSourceRanges:
    ##   - 10.10.10.0/24
    ##
    loadBalancerSourceRanges: []
    ## @param service.externalTrafficPolicy runner service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param service.annotations Additional custom annotations for runner service
    ##
    annotations: {}
    ## @param service.extraPorts Extra ports to expose in runner service (normally used with the `sidecars` value)
    ##
    extraPorts: []
    ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## @param runner.containerPorts.http runner HTTP container port
    ## @param runner.containerPorts.https runner HTTPS container port
    ##
  containerPorts:
    http: 3006
    https: 3006
  ## @param runner.extraContainerPorts Optionally specify extra list of additional ports for runner containers
  ## e.g:
  ## extraContainerPorts:
  ##   - name: myservice
  ##     containerPort: 9090
  ##
  extraContainerPorts: []
  updateStrategy:
    type: "RollingUpdate"
  ## Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
  ## @param runner.pdb.create Enable/disable a Pod Disruption Budget creation
  ## @param runner.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
  ## @param runner.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `runner.pdb.minAvailable` and `runner.pdb.maxUnavailable` are empty.
  ##
  pdb:
    create: false
    minAvailable: ""
    maxUnavailable: ""
  ## Autoscaling configuration
  ## ref: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  ##
  autoscaling:
    ## @param runner.autoscaling.hpa.enabled Enable HPA for runner pods
    ## @param runner.autoscaling.hpa.minReplicas Minimum number of replicas
    ## @param runner.autoscaling.hpa.maxReplicas Maximum number of replicas
    ## @param runner.autoscaling.hpa.targetCPU Target CPU utilization percentage
    ## @param runner.autoscaling.hpa.targetMemory Target Memory utilization percentage
    ##
    hpa:
      enabled: true
      minReplicas: 1
      maxReplicas: 3
      targetCPU: 75
      targetMemory: 75
    ## @param runner.autoscaling.vpa.enabled Enable VPA for runner pods
    ## @param runner.autoscaling.vpa.annotations Annotations for VPA resource
    ## @param runner.autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
    ## @param runner.autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
    ## @param runner.autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
    ##
    vpa:
      enabled: false
      annotations: {}
      controlledResources: []
      maxAllowed: {}
      minAllowed: {}
      ## @param runner.autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy
      ## Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
      ## Possible values are "Off", "Initial", "Recreate", and "Auto".
      ##
      updatePolicy:
        updateMode: Auto
  ## @param runner.sidecars Add additional sidecar containers to the runner pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## runner resource requests and limits
  ## ref: http://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param runner.resourcesPreset Set runner container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if runner.resources is set (runner.resources is recommended for production).
  ##
  resourcesPreset: "small"
  ## @param runner.resources Set runner container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param runner.extraEnvVars Array with extra environment variables to add to runner containers
  ## e.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: "bar"
  ##
  extraEnvVars: []
  ## @param runner.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for runner containers
  ##
  extraEnvVarsCM: ""
  ## @param runner.extraEnvVarsSecret Name of existing Secret containing extra env vars for runner containers
  ##
  extraEnvVarsSecret: ""
  ## Configure extra options for runner containers' liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param runner.livenessProbe.enabled Enable livenessProbe on runner containers
  ## @param runner.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param runner.livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param runner.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param runner.livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param runner.livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1
  ## @param runner.readinessProbe.enabled Enable readinessProbe on runner containers
  ## @param runner.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param runner.readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param runner.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param runner.readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param runner.readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 2
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  ## @param runner.startupProbe.enabled Enable startupProbe on runner containers
  ## @param runner.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param runner.startupProbe.periodSeconds Period seconds for startupProbe
  ## @param runner.startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param runner.startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param runner.startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 24
    successThreshold: 1
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param runner.podSecurityContext.enabled Enable runner pods' Security Context
  ## @param runner.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy for runner pods
  ## @param runner.podSecurityContext.sysctls Set kernel settings using the sysctl interface for runner pods
  ## @param runner.podSecurityContext.supplementalGroups Set filesystem extra groups for runner pods
  ## @param runner.podSecurityContext.fsGroup Set fsGroup in runner pods' Security Context
  ##
  podSecurityContext:
    enabled: false
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## @param runner.deploymentAnnotations Annotations for runner deployment
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  deploymentAnnotations: {}
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
  ##
  persistence:
    ## @param persistence.enabled Enable persistence using Persistent Volume Claims
    ##
    enabled: false
    ## @param persistence.mountPath Path to mount the volume at.
    ##
    mountPath: /nango/runner/data
    ## @param persistence.subPath The subdirectory of the volume to mount to, useful in dev environments and one PV for multiple services
    ##
    subPath: ""
    ## @param persistence.storageClass Storage class of backing PVC
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size Size of data volume
    ##
    size: 8Gi
    ## @param persistence.dataSource Custom PVC data source
    ##
    dataSource: {}
    ## @param persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: ""
    ## @param persistence.selector Selector to match an existing Persistent Volume 
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: {}
  ## RBAC configuration
  ##
  rbac:
    ## @param rbac.create Specifies whether RBAC resources should be created
    ##
    create: false
    ## @param rbac.rules Custom RBAC rules to set
    ## e.g:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##
    rules: []

redis:  
  enabled: true
  primary:
    persistence:
      enabled: false
    resources:
      limits:
        cpu: "1000m"
        memory: "2048Mi"
      requests:
        cpu: "250m"
        memory: "1024Mi"

postgresql:
  namespace: nango
  enabled: true
  primary:
    persistence:
      enabled: false
    resources:
      limits:
        cpu: "1000m"
        memory: "2048Mi"
      requests:
        cpu: "250m"
        memory: "1024Mi"
  auth:
    postgresPassword: nango
    database: nango

elasticsearch:
  namespace: nango
  enabled: false
  clusterName: elastic
  # single node cluster
  master:
    masterOnly: false
    replicaCount: 1
  data:
    replicaCount: 0
  coordinating:
    replicaCount: 0
  ingest:
    replicaCount: 0
  security:
    enabled: true
    elasticPassword: nango